<h1 align="center">🚀 SwiftLet</h1>

<p align="center"><strong>SwiftLet is a lightweight Python framework for running open-source Large Language Models (LLMs) locally using <code>safetensors</code>.</strong></p>

<p>
SwiftLet provides a minimal and efficient interface to load and execute LLMs on your own hardware. It supports models in the <code>safetensors</code> format and is designed to be simple, fast, and easy to use—without any external dependencies or cloud-based services.
</p>

<p>
This project is currently in its early stages and focuses on enabling direct local execution of LLMs. Future updates will expand its capabilities with support for advanced features such as tool integration, file interaction, and modular extensions to improve local LLM workflows.
</p>

## 🛣️ Planned Features

SwiftLet is under active development. The following features are planned for future releases:

- 🔧 **Integration with Native Runtimes**  
  Support for running LLMs via optimized backends like `llama.cpp` for improved performance on local machines.

- 📂 **File Interaction Support**  
  Enable LLMs to read and process local documents, files, and structured data formats.

- 🔌 **Modular Tool Integration**  
  Easily connect models to external tools, functions, or APIs to extend their utility.

- 🧠 **Enhanced Model Management**  
  Tools to manage multiple models, switch between them, and handle custom configurations.

- 📜 **Prompt Templates and Inference APIs**  
  Built-in support for structured prompting, templates, and customizable inference pipelines.

- ⚙️ **Extensibility and Plugins**  
  A modular architecture that allows developers to add new capabilities with simple plugin hooks.

- 📊 **Performance Monitoring and Debugging**  
  Tools for logging, inspecting model behavior, and optimizing local performance.

---

> 💡 *If you're interested in contributing or testing early features, stay tuned for upcoming releases and roadmap updates.*
